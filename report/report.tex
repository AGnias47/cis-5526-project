\documentclass[11pt,letterpaper,oneside,titlepage]{article}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}
\usepackage[margin=1in]{geometry}
    \setlength{\parindent}{0em}
    \setlength{\parskip}{1em}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\usepackage{enumerate}
\usepackage{enumitem}
\setlist[itemize]{topsep=0pt}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{listings}
\usepackage{verbatim}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.png}
\graphicspath{ {images/} }
\usepackage{placeins}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{amsmath}  % For mathematical symbols and equations
\usepackage{amsfonts} % For mathematical fonts
\usepackage{amssymb}  % For additional mathematical symbols
\usepackage{csquotes} % For quotes
\usepackage{caption}
\usepackage{enumitem} % For customizing lists
\usepackage{graphicx}       % graphics
\usepackage{listings}
\usepackage{xcolor}
\usepackage{placeins} % For FloatBarrier
\usepackage{float}
\usepackage{booktabs}       % professional-quality tables
\usepackage{hyperref} % For references
\usepackage{tikz}
\usetikzlibrary{trees}
\usetikzlibrary{shapes}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\fontsize{12}{14}\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}
\usepackage[
backend=biber,
style=numeric,
]{biblatex}
\addbibresource{references.bib}

% From https://www.overleaf.com/learn/latex/Code_listing#Importing_code_from_a_file
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\title{Predicting IMDB Movie Ratings}
\author{Andy Gnias}

\begin{document}
\maketitle

\section{Introduction}

The ability to predict how a movie will rank among audiences before it is seen is beneficial to several stakeholders. If a video streaming service is deciding whether to host a new movie, audience favorability will hold significant value. One metric of audience ranking is a movie's rating on the Internet Movie Database, referred to as IMDB. IMDB utilizes user rankings to give movies a rating from 1 to 10 with 0.1 granularity. The goal of this project will be to create a model that can accurately predict the IMDB ranking of a movie.

Movie metadata data will be used to generate features. On Kaggle, user OctopusTeam has uploaded datasets containing movie data from streaming services Netflix, Apple TV, Amazon Prime, Hulu, and Max \cite{OctopusTeam_2024}. In total, this gives us a dataset of around 67,000 movies before preprocessing. New features can also be added from other external sources. When adding features, features that could indicate a movie's success or failure after release were not used. For example, gross revenue would be a good indicator of a movie's success, but in a real world example, if revenue can be found, critical reviews can also be found. The real value in the model described in this project would be the ability to predict a movie's ranking before an audience as seen and critiqued it.

This is a regression problem. Although it can be modeled as a classification problem, ex. above or below a certain rating, the goal of this project will be to see how close the model can get to predicting a movie's actual IMDB rating. In attempting to solve this problem, several different approaches will be tested: linear regression, ensemble methods, SVMs, and neural networks. Models for each approach will be developed, and results of each model will be compared.

\section{Data Analysis}

The dataset was initially seeded with 5 CSV files from Kaggle user OctopusTeam\cite{OctopusTeam_2024}. These CSVs contained movies and TV shows from major streaming services. The fields included Title, Type (movie or TV show), Genre, Release Year, IMDB ID, IMDB Rating, IMDB Number of Votes, and Available Countries. Title was used as an identifier, and anything without a title was dropped. Type was used to filter out TV shows. IMDB Rating was used as the label, and anything with a 0 was dropped, as that indicated that no rating was present. IMDB Number of Votes and Available Countries were disregarded, as they could indicate the movie's success after release. This left us with Genre and Release Year as features. Genre was a comma-separated list. This was one-hot encoded, where for each genre the movie was classified as, a 1 would be added to that category's column. Release year was normalized via Z-score and used as a feature, and any sample without a release year was dropped. Any duplicates were removed, as there was likely overlap in movies between the 5 streaming services.

These features seemed insufficient for a robust model. However, with the IMDB ID, the IMDB page for each movie could be found. Initially, issuing a Python GET request to these pages gave a 403 error, likely due to the site's anti-scraping. These were worked around by using a Python requests Session object that included a User-Agent header, making it seem more like a browser, and a rate-limiting mechanism to not overload the site or seem like bot traffic. Implementing these measures allowed the following features to be scraped from IMDB directly: Director, Runtime, Content Rating (ex. G, PG, PG-13, R), and Description. Runtime was converted to minutes and normalized via z-score, and Content Rating was 1-hot encoded. From here, a Baseline dataset was defined with 60,940 samples and 62 features.

2 separate expansions to this dataset were generated. In one expansion, the directors of each movie were 1-hot encoded. However, this resulted in 31,237 features, which made its use impractical for most models. Another expansion was to perform sentiment analysis via the Hugging Face Transformers library on the description of each movie, and utilize the sentiment score as a feature. For example, a movie with an extremely positive description would have a sentiment score of 1, and a movie with an extremely negative description would have a sentiment score of -1.

Each dataset underwent a 70-15-15 split between train, validation, and test datasets, where validation is used after a training run for a model is complete, and testing is run as a separate step on a trained model. Ordering is deterministic within, but not across models (ex. same order for all linear regression runs, but different order from any neural network runs).

\begin{center}
\begin{tabular}{|l|l|l|l|l|l|l|l}
\cline{1-7}
          \textbf{Dataset} & \textbf{Year}    & \textbf{Genre}   & \textbf{Content Rating} & \textbf{Runtime} & \textbf{Director} & \textbf{Sentiment of Desc}    &  \\ \cline{1-7}
Baseline  & z-score & One-hot & One-hot        & z-score &          &                             &  \\ \cline{1-7}
Directors & z-score & One-hot & One-hot        & z-score & One-hot  &                             &  \\ \cline{1-7}
Sentiment & z-score & One-hot & One-hot        & z-score &          & Raw (by default {[}-1,1{]}) &  \\ \cline{1-7}
\end{tabular}
    \captionsetup{justification=centering}
    \captionof{table}{Features included in each dataset with normalization method. If blank, not included}
\end{center}

\section{Methodology}

Several regression methods will be attempted on the datasets. Each model will be evaluated by its Mean Squared Error as well as R$^2$ score. The R$^2$ score can indicate the efficacy of a regression model. 1 indicates a perfect model, 0 indicates a model returning the average of all labels, and negative indicates that the model is worse than average\cite{sklearn_r2}\cite{Wall_2022}. Additionally, a pseudo-random model was written to give a comparative baseline against randomly picking a rating for each movie. The models that will be evaluated are described below.

\subsection{Linear Regression}

Linear regression is less effective for complex data. However, it is simple to implement and run, and gives a good comparative baseline. Simpler models can be more resistant to over-fitting, use less computational power, and be more easily understood to external stakeholders. If a linear model performs well compared to something more complex, like a neural network, there are valid arguments for using the linear model.

Linear regression algorithms will be written with NumPy to implement equations used in class. Several methods will be implemented, including solving for the closed form as

\begin{equation}
    W^* = (X^{\top}X)^{-1}\cdot X^{\top}Y
\end{equation}

This equation can also be expanded to implement Ridge Regression as
\begin{equation}
    W^* = (X^{\top}X + \lambda I)^{-1}\cdot X^{\top}Y\ \text{where}\ \lambda=0.1
\end{equation}

Additionally, gradient descent, mini-batch gradient descent, and stochastic gradient descent will be implemented.

All of these models will be trained on the Baseline dataset. For the Directors dataset, only mini-batch gradient descent will be used, as its size, 1.8GB, only allows for loading it in chunks. Fortunately, chunking data coincides with how mini-batch runs. For the Sentiment dataset, only the gradient descent methods will be used. An inverse cannot be found for the X matrix, so the the analytical methods cannot be used.

\subsection{scikit-learn models: Random Forest, SVC, and XGBoost}

Ensemble methods such as Random Forests and XGBoost are known to be effective for regression models\cite{Keboola_2020}\cite{Snowflake}\cite{mccurciomccurcio}. Initially, only a Random Forest model was going to be tested. However, the scikit-learn library implements other models that are effective for regression utilizing nearly identical code, so it was trivial to also test SVC and XGBoost. The code written for these models is limited to configuring the datasets and evaluating the models.

% describe more about them here

\subsection{Neural Network}

A Feedforward Neural Network was also developed. While not as commonly used in regression, they can still be highly effective in solving regression problems. The network was developed using the PyTorch library, using the neural network given in Homework 5 as a starting point to build off of. Data was loaded via a custom-generated DataLoader for the project data.

The neural network used 25 epochs with a learning rate of 0.01. The data was loaded with a batch size of 64. The neural network consisted of 5 layers of neuron size 62 (63 for the Sentiment dataset), 45, 23, 12, and 1 for the output which represented the predicted rating. All layers were linear and used ReLU for the activation function. MSE was the loss function, and the Adam optimizer was used. An Exponential Learning rate scheduler was used to modify the learning rate as training was performed. Training was performed on a privately owned Linux PC with a GeForce RTX 3060 Ti 8GB GPU.

\section{Results}

\begin{center}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Models with Baseline Data}                                & \multicolumn{1}{l|}{\textbf{MSE}}     & \multicolumn{1}{l|}{\textbf{R2}}       \\ \hline
Random Rating Generator                & \multicolumn{1}{l|}{8.59662} & \multicolumn{1}{l|}{-4.19021} \\ \hline
Linear Regression - Closed Form      & 4.44555                      & -1.62475                      \\ \hline
Linear Regression - Ridge Regression & 4.4493                       & -1.62696                      \\ \hline
Linear Regression - Gradient Descent & 4.54581                      & -1.68394                      \\ \hline
Linear Regression - Mini Batch       & 4.45112                      & -1.62804                      \\ \hline
Linear Regression - SGD              & 4.83463                      & -1.85447                      \\ \hline
Random Forest                        & 1.04926                      & 0.38050                       \\ \hline
SVC                                  & 1.03277                      & 0.39023                       \\ \hline
XGBoost                              & 1.03904                      & 0.38653                       \\ \hline
FNN                                  & 0.99080                      & 0.39958                       \\ \hline
\end{tabular}
    \captionsetup{justification=centering}
    \captionof{table}{Results from Baseline dataset}\label{baseline}
\end{center}
\FloatBarrier

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Model with Directors Data} & \textbf{MSE}                 & \textbf{R2}                   \\ \hline
Linear Regression - Mini Batch     & \multicolumn{1}{r|}{3.73268} & \multicolumn{1}{r|}{-1.24715} \\ \hline
\end{tabular}
    \captionsetup{justification=centering}
    \captionof{table}{Results from Directors dataset}\label{directors}
\end{center} \label{baseline}
\FloatBarrier

\begin{center}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Model with Sentiment Data}                                     & \multicolumn{1}{l|}{\textbf{MSE}} & \multicolumn{1}{l|}{\textbf{R2}} \\ \hline
Linear Regression - Gradient Descent           & 4.43741                  & -1.64103                \\ \hline
Linear Regression - Mini Batch                 & 4.52935                  & -1.69575                \\ \hline
Linear Regression - SGD & 4.70433                  & -1.79989                \\ \hline
RandomForest                              & 1.08369                  & 0.35502                 \\ \hline
SVR                                       & 1.08148                  & 0.35633                 \\ \hline
XGBoost                                   & 1.08105                  & 0.35659                 \\ \hline
FNN                                       & 1.12319                  & 0.31382                 \\ \hline
\end{tabular}
    \captionsetup{justification=centering}
    \captionof{table}{Results from Sentiment dataset}\label{sentiment}
\end{center} \label{baseline}
\FloatBarrier

\section{Discussion}

% Describe how closed form performed better than GD, shows why preferred when possible. only improvement was with ridge regression. describe how benefitted from the use of director data. slower, but allowed for more accuracy.

\section{Conclusion}

% compare models
% future work: better feature selections, ex. something as effective as directors that isn't as costly. Hyperparameter tuning.

\newpage

\printbibliography

\newpage

\section{Appendix}

\subsection{Code}

All code can be accessed at \href{https://github.com/AGnias47}{https://github.com/AGnias47}.


\subsection{Hardware}

A privately owned Linux PC with a GeForce RTX 3060 Ti 8GB GPU was used to perform all experiments described in this paper.

\subsection{Acknowledgements}


\subsection{gemini questions}

out features are neurons
\end{document}
